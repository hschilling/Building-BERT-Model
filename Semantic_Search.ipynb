{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic-Search.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vizzies/Building-BERT-Model/blob/master/Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG306zAvbptK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(‘/content/gdrive’)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrhpWn-ObwQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU Setup\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5DnwN-Xb0ta",
        "colab_type": "text"
      },
      "source": [
        "# Import Data Below and Parse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2Rflxlhb1Mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "\n",
        "with open('/content/arc-code-ti-publications.pkl', 'rb') as f:\n",
        "    pubs = pandas.read_pickle(f)\n",
        "\n",
        "import re\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "    \n",
        "def preprocess_text(sen):\n",
        "\n",
        "    sentence = str(sen)\n",
        "\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sentence)\n",
        "\n",
        "    # Remove hyphenation if at the end of a line\n",
        "    sentence = sentence.replace('-\\n', '')\n",
        "\n",
        "    # Fix ligatures\n",
        "    sentence = unicodedata.normalize(\"NFKD\", sentence)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "# Not really needed any more but will leave in and just comment out\n",
        "# full_texts = []\n",
        "# sentences = list(pubs['Text'])\n",
        "# for sen in sentences:\n",
        "#     full_texts.append(preprocess_text(str(sen)))\n",
        "\n",
        "pubs['Text Processed'] = pubs.apply(lambda row: preprocess_text(row['Text']), axis=1)\n",
        "\n",
        "text_df = pubs[['Text Processed',]].copy()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwtovYHgcObq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucmaa9IScZjU",
        "colab_type": "text"
      },
      "source": [
        "## BERT Sentence Tranformers Semantic Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jVvreBycPSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This is a simple application for sentence embeddings: semantic search\n",
        "We have a corpus with various sentences. Then, for a given query sentence,\n",
        "we want to find the most similar sentence in this corpus.\n",
        "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
        "\"\"\"\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import scipy.spatial\n",
        "import pickle as pkl\n",
        "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Eaxmple query sentences\n",
        "queries = []\n",
        "query_embeddings = embedder.encode(queries,show_progress_bar=True)\n",
        "\n",
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "closest_n = 5\n",
        "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n=========================================================\")\n",
        "    print(\"==========================Query==============================\")\n",
        "    print(\"===\",query,\"=====\")\n",
        "    print(\"=========================================================\")\n",
        "\n",
        "\n",
        "    for idx, distance in results[0:closest_n]:\n",
        "        print(\"Score:   \", \"(Score: %.4f)\" % (1-distance) , \"\\n\" )\n",
        "        print(\"Paragraph:   \", corpus[idx].strip(), \"\\n\" )\n",
        "        row_dict = df.loc[df.index== corpus[idx]].to_dict()\n",
        "        ## print(\"paper_id:  \" , row_dict[\"paper_id\"][corpus[idx]] , \"\\n\")\n",
        "        print(\"Title:  \" , row_dict[\"title\"][corpus[idx]] , \"\\n\")\n",
        "        print(\"Abstract:  \" , row_dict[\"abstract\"][corpus[idx]] , \"\\n\")\n",
        "        print(\"Abstract_Summary:  \" , row_dict[\"abstract_summary\"][corpus[idx]] , \"\\n\")\n",
        "        print(\"-------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}