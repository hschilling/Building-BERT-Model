{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building_BERT_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vizzies/Building-BERT-Model/blob/shruti-bert-work/Building-BERT-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciRP6XqLrHP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(‘/content/gdrive’)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2N9_60snU7R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6159b7f6-eaae-4e5d-e4c6-161ba2552063"
      },
      "source": [
        "# GPU Setup\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB4DwkIwodyy",
        "colab_type": "text"
      },
      "source": [
        "# Import Data Below and Parse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFD53HqLnYoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "\n",
        "with open('/content/arc-code-ti-publications.pkl', 'rb') as f:\n",
        "    pubs = pandas.read_pickle(f)\n",
        "\n",
        "import re\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "    \n",
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "full_texts = []\n",
        "sentences = list(pubs['Text'])\n",
        "for sen in sentences:\n",
        "    full_texts.append(preprocess_text(str(sen)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiK4xOKFoypg",
        "colab_type": "text"
      },
      "source": [
        "## Bert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBX1Q3Kfo1Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsayLg-xo6y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply tokenizer\n",
        "\n",
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBNrUX9uvwNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize entire dataset\n",
        "\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add CLS and SEP tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNlxZnv-vzOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in sentences:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "      sent,\n",
        "      add_special_toekns = True, # adds CLS and SEP tokens\n",
        "      max_length = 64,\n",
        "      pad_to_max_length = True,\n",
        "      return_attention_mask = True, # Creates attention masks\n",
        "      return_tensors = 'pt', #pytorch\n",
        "  )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# convert lists to tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])gr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOf58Su03fbA",
        "colab_type": "text"
      },
      "source": [
        "## Split dataset\n",
        "90% for training, 10% for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Vil7qw3mrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyK2ABQW_VPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset iterator to save memory during training\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# 16 or 32 is ok\n",
        "batch_size = 32\n",
        "\n",
        "# dataloaders for training and validation in random order\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "# sequential reading for validation\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}